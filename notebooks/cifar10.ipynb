{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        normalization: nn.Module = nn.Identity,\n",
    "        activation: nn.Module = nn.Identity(),\n",
    "    ):\n",
    "        \"\"\"Convolutional block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Input channels.\n",
    "            out_channels (int): Output channels.\n",
    "            kernel_size (int): Kernel size.\n",
    "            stride (int): Stride value.\n",
    "            padding (int): Padding value.\n",
    "            normalization (nn.Module): Noralization we want to use. Defaults to nn.Identity.\n",
    "            activation (nn.Module): Activation function, it should be already instantiated. Defaults to nn.Identity().\n",
    "        \"\"\"\n",
    "        super(Conv, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=[kernel_size, kernel_size],\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # In GAN's seminal paper is advised to use a normal distribution.\n",
    "        torch.nn.init.normal_(self.conv.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(self.conv.bias, val=0.0)\n",
    "\n",
    "        self.norm = normalization(out_channels)\n",
    "\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        conditional_dim: int,\n",
    "        filters: List[int],\n",
    "        out_channels: int,\n",
    "    ):\n",
    "        \"\"\"Discriminator.\n",
    "\n",
    "        Args:\n",
    "            noise_dim (int): Number of input channels.\n",
    "            conditional_dim (int): Dimension of the conditional vector.\n",
    "            filters (List[int]): List of feature map dimensions.\n",
    "            out_channels (int): Output image channels.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Layers for separated inputs.\n",
    "        self.conv_x = Conv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=filters[0] // 2,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            activation=nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.conv_c = Conv(\n",
    "            in_channels=conditional_dim,\n",
    "            out_channels=filters[0] // 2,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            activation=nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # Layers for combined inputs.\n",
    "        conv_xc = []\n",
    "        for i in range(1, len(filters)):\n",
    "            conv_xc.append(\n",
    "                Conv(\n",
    "                    in_channels=filters[i - 1],\n",
    "                    out_channels=filters[i],\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    normalization=nn.BatchNorm2d,\n",
    "                    activation=nn.LeakyReLU(0.2),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output layer.\n",
    "        conv_out = Conv(\n",
    "            in_channels=filters[-1],\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            activation=nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_xc, conv_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            c (torch.Tensor): Conditional tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Probability distribution over real/fake output.\n",
    "        \"\"\"\n",
    "        out_x = self.conv_x(x)\n",
    "        out_c = self.conv_c(c)\n",
    "        # Combine input and conditional informations.\n",
    "        out = torch.cat((out_x, out_c), dim=1)\n",
    "        out = self.conv(out)\n",
    "        # Reshape to b, output_dim.\n",
    "        out = out.view(out.shape[0], out.shape[1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        normalization: nn.Module = nn.Identity,\n",
    "        activation: nn.Module = nn.Identity(),\n",
    "    ):\n",
    "        \"\"\"Transposed convolution block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Input channels.\n",
    "            out_channels (int): Output channels.\n",
    "            kernel_size (int): Kernel size.\n",
    "            stride (int): Stride value.\n",
    "            padding (int): Padding value.\n",
    "            normalization (nn.Module): Noralization we want to use. Defaults to nn.Identity.\n",
    "            activation (nn.Module): Activation function, it should be already instantiated. Defaults to nn.Identity().\n",
    "        \"\"\"\n",
    "        super(ConvT, self).__init__()\n",
    "\n",
    "        self.convT = nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=[kernel_size, kernel_size],\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # In GAN's seminal paper is advised to use a normal distribution.\n",
    "        torch.nn.init.normal_(self.convT.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(self.convT.bias, val=0.0)\n",
    "\n",
    "        self.norm = normalization(out_channels)\n",
    "\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        out = self.convT(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        noise_dim: int,\n",
    "        conditional_dim: int,\n",
    "        filters: List[int],\n",
    "        out_channels: int,\n",
    "    ):\n",
    "        \"\"\"Generator.\n",
    "\n",
    "        Args:\n",
    "            noise_dim (int): Dimension of the sampled noise.\n",
    "            conditional_dim (int): Dimension of the conditional vector.\n",
    "            filters (List[int]): List of feature map dimensions.\n",
    "            out_channels (int): Output image channels.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Layers for separated inputs.\n",
    "        self.convT_z = ConvT(\n",
    "            in_channels=noise_dim,\n",
    "            out_channels=filters[0] // 2,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            normalization=nn.BatchNorm2d,\n",
    "            activation=nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.convT_c = ConvT(\n",
    "            in_channels=conditional_dim,\n",
    "            out_channels=filters[0] // 2,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            normalization=nn.BatchNorm2d,\n",
    "            activation=nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Layers for combined inputs.\n",
    "        convT_zc = []\n",
    "        for i in range(1, len(filters)):\n",
    "            convT_zc.append(\n",
    "                ConvT(\n",
    "                    in_channels=filters[i - 1],\n",
    "                    out_channels=filters[i],\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    normalization=nn.BatchNorm2d,\n",
    "                    activation=nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output layer.\n",
    "        convT_out = ConvT(\n",
    "            in_channels=filters[-1],\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            activation=nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.convT = nn.Sequential(*convT_zc, convT_out)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): Noise tensor.\n",
    "            c (torch.Tensor): Conditional tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated image.\n",
    "        \"\"\"\n",
    "        out_z = self.convT_z(z)\n",
    "        out_c = self.convT_c(c)\n",
    "        # Combine input and conditional informations.\n",
    "        out = torch.cat((out_z, out_c), dim=1)\n",
    "        out = self.convT(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
