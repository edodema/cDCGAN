\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

% \def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{cDCGAN: conditional Deep Convolutional Generative Adversarial Networks}

\author{Edoardo De Matteis\\
La Sapienza University of Rome\\
{\tt\small dematteis.1746561@studenti.uniroma1.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   GANs have been described as one of the most interesting idea in the last years of machine learning and have been integrated with CNNs to have strong priors on images.
   Conditional GANs allow us to condition generation through supplementary input, in this project I combine both these models in a conditional deep convolutional GAN, code is available at \href{https://github.com/edodema/cDCGAN}{https://github.com/edodema/cDCGAN}.
\end{abstract}

\section{Introduction}
Since they were first introduced GANs ~\cite{goodfellow2014generative} have gained so much popularity to the point that Yann LeCun considers them the most interesting idea in the last 10 years of machine learning ~\cite{lecun2016riseminar}.
Their ability to generate realistic images has amazed both specialists and the general public, e.g. the website \textit{This Person Does Not Exist} \footnote{\href{https://thispersondoesnotexist.com/}{https://thispersondoesnotexist.com/}} that is based on \textit{StyleGAN2} ~\cite{viazovetskyi2020stylegan2}, yet it has been recently overcome by \textit{StyleGAN3} ~\cite{DBLP:journals/corr/abs-2106-12423}.
Both developed by \textit{NVIDIA}, the latter was trained for a complexive time of 96 years\footnote{It was trained on multiple GPUs at a time.},
that number casts a light on one big issue: GANs are very hard to train and rely a lot on GPUs.


\section{Related works}
\paragraph{VAE} We call autoencoder (AE) any model that outputs a reconstruction of the input.
Tipically defined by an encoder-decoder architecture, we call the bottleneck \textit{latent representation}.
The problem with AEs is that they can map similar inputs to arbitarily distant regions of the latent space, but we want to learn a distribution.
A \textit{variational autoencoder} (VAE) ~\cite{kingma2013auto} constructs the parameters of a probability distribution on the latent space, with the distribution being fixed a priori (often a Gaussian) and training data being seen as a sampling of the learned distribution.

\paragraph{GAN} The idea behind \textit{generative adversarial networks} ~\cite{goodfellow2014generative} is based on VAEs: we can split the architecture and call the encoder \textbf{discriminator} and the decoder \textbf{generator}.
Yet they are not the same: the decoder's output is a probability of its input being real, way different from generator's input i.e. a sample from a probability distribution (often Gaussian).
Discriminator and generator play a zero-sum game where the generator's goal is to fool the discriminator with fake data, and the discriminator wants to catch the generator.

\paragraph{DCGAN} GANs were introduced based on \textit{multi-layer perceptrons} only, it came natural to extend them with CNNs and exploit priors on images.
In ~\cite{radford2015unsupervised} are laid down some guidelines for a more stable convolutional GAN:
\begin{itemize}
   \item Replace deterministic spatial pooling with strided convolutions, allowing the network to learn its own downsampling or upsampling functions.
   \item Remove fully connected layers on top of CNNs (e.g. classification heads) in favor of deeper architectures, preferring one-dimensional convolutions and vector transformations.
   \item Employ batch normalization in both the discriminator and the generator, yet too many can make the latter unstable.
   \item Use ReLU activations in the generator for all layers except the output, in which there is a $\tanh$ function.
         In the discriminator LeakyReLU is preferred.
\end{itemize}

\paragraph{cGAN} Until now GANs generate data with no consideration for classes, ~\cite{mirza2014conditional} describes a model that receives some conditioning along with data and outputs class specific images.
Here it is a one hot vector but can potentially be anything.

\subsection{Task and goals}
In this work I aim to combine a DCGAN based structure with some conditioning, thus creating a cDCGAN.
Class conditional synthesis can significantly improve the quality of generated samples ~\cite{DBLP:journals/corr/OordKK16}, and such model could be used for specialized data enrichment or could have applications in entertainment (e.g. \textit{FaceApp}).
Its development could also lead to advancements in other branches of machine learning e.g. injecting natural language knowledge as conditional data.

\section{Methodology}

\subsection{Dataset and preprocessing}
The model has been trained on three different datasets: \textit{MNIST}, \textit{FashionMNIST} and \textit{CIFAR10}, data are $z$-score normalized with specific mean and standard deviation previously computed on each dataset due to being easier to learn on data with $0$ mean and $1$ standard deviation.
Images are reshaped to be $64\times64$ by default, this way we can test the same model on each dataset and while in general we can learn more on high resolution data it could also be that upsampling procedures add noise.

Conditioning is embedded as a one-hot vector, manipulated such that it could be added as a $64\times64$ matrix to the image through concatenation as an additional channel.
Noise samples have a size of $64$ as well.

\subsection{Model}
\paragraph{Discriminator}
The discriminator is based on a basic block composed of a convolution, a batch normalization and a LeakyReLU activation function whose negative slope equals to $0.2$.
The discriminator model is defined as:
\begin{enumerate}
   \item Concatenation of input image and its conditioning on the channel dimension.
   \item Convolution with LeakyReLU activation.
   \item Basic block defined above, repeated three times.
   \item Convolution and sigmoid function.
\end{enumerate}
The model is shown in figure \ref{fig:discriminator}, implementation details can be seen on the official repo.

\begin{figure}[h!t]
   \centering
   \includegraphics[scale=0.25]{images/discr.png}
   \caption{Discriminator scheme.}
   \label{fig:discriminator}
\end{figure}

\paragraph{Generator}
Generator's basic block is similiar to discriminator's one, yet instead of a convolution we employ transpose convolution for upsampling, and we use ReLU as activation function.
The model is in a way simpler than the previous one:
\begin{enumerate}
   \item A sampled noise matrix is concatenated to its conditioning.
   \item We repeat generator's basic block 4 times.
   \item The model ends with a transpose convolution and a $\tanh$ activation function.
\end{enumerate}
The model is shown in figure \ref{fig:generator}, code is available on the code linked in the abstract.

\begin{figure}[h!t]
   \centering
   \includegraphics[scale=0.25]{images/gen.png}
   \caption{Generator scheme.}
   \label{fig:generator}
\end{figure}

\subsection{Loss}
In their seminal paper ~\cite{goodfellow2014generative}, GANs' loss is defined as a minmax loss (\ref{eq:minmax}), where $p_d$ and $p_z$ represent the distribution over real and fake data respectively.
This is quite incovenient and in the same paper it is approximated to a easier loss.
\begin{equation}
   \label{eq:minmax}
   \min_\gamma \max_\delta \mathbb{E}_{p_d} \log D(x) + \mathbb{E}_{p_z} \log (1-D(G(z)))
\end{equation}
Another approach consists in using a classical binary cross entropy loss \ref{eq:bce}, one for the discriminator and generator each.
\begin{equation}
   \label{eq:bce}
   -\frac{1}{N} \sum_{i=1}^N y_i \log (f(x_i)) + (1-y_i) \log (1-f(x_i))
\end{equation}
Discriminator's ground truth is $\vec{0}$ for fake and $\vec{1}$ for real data.
The generator would maximize on fake data i.e. $\vec{0}$ yet it is easier to minimize on $\vec{1}$ thus we flip zeros with ones.

\section{Experiments and results}
For the MNIST and FashionMNIST datasets the network has been trained for 5 and 10 epochs respectively with a learning rate equal to $2\times10^{-4}$ and results are overall good.
The same cannot be said for CIFAR10, trained for 25 epochs with $2\times 10^{-4}$ and $2\times10^{-5}$ learning rates for discriminator and generator respectively; we can see some familiar and sensible shapes, yet images are not realistic at all.
This makes sense since CIFAR10 scenes are way more complicated than MNIST and FashionMNIST's, also having 3 channels instead of 1 can make things harder for the model.

\paragraph{Weighting}
Weighting conditioning can give us interesting results, on the MNIST and FashionMNIST it enhances contrast and since images are black and white it translates to clearer outputs.

\paragraph{Multiclass conditioning}
Conditioning does not necessarily have to be a one hot vector, if more indices are equal to 1 the generator will morph images from more classes.
We can assign values different from one, even decimal, depending on what we want to create and with which strength.

\section{Conclusions}
I developed a conditional GAN that exploits convolutions, considering the relatively short training time it is quite good on simple datasets.
If the dataset gets more complicated results do not stand out, a plausible reason is that training GANs, other than being notoriously hard, requires large computations.

\paragraph{Future works} For this project I only had a free GPU offered by \textit{Google Colab}, it would be interesting to see how the model does when trained properly.
Another interesting approach could be to plug in classification pretrained models as a feature extraction backbone and fine tune them.

\begin{figure}
   \centering
   \includegraphics[scale=0.4]{images/mnist.png}
   \caption{MNIST generated images.}
   \label{fig:mnist}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.4]{images/fashion.png}
   \caption{FashionMNIST generated images.}
   \label{fig:fashion}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/cifar.png}
   \caption{CIFAR10 generated images.}
   \label{fig:cifar}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/mnist-weighted.png}
   \caption{MNIST predictions with weighted conditioning vector.}
   \label{fig:mnist-weight}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/fashion-weighted.png}
   \caption{FashionMNIST predictions with weighted conditioning vector.}
   \label{fig:fashion-weight}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/gen35.png}
   \caption{MNIST prediction of both $3$ and $5$.}
   \label{fig:mnist-35}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/gen35with3weighted.png}
   \caption{MNIST prediction of both $3$ and $5$, with $3$ being weighted more.}
   \label{fig:mnist-3w5}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/gen35with5weighted.png}
   \caption{MNIST prediction of both $3$ and $5$, with $5$ being weighted more.}
   \label{fig:mnist-35w}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/shirt.png}
   \caption{FashionMNIST prediction of a shirt.}
   \label{fig:fashion-shirt}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/pants.png}
   \caption{FashionMNIST prediction of a pair of pants.}
   \label{fig:fashion-pants}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[scale=0.39]{images/shirtpants-weighted.png}
   \caption{FashionMNIST prediction of both shirt and pants, in this example pants are weighted $0.7$ otherwise they tend to hog the scene.}
   \label{fig:fashion-shirt-pants}
\end{figure}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
